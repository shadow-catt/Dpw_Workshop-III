{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use NLTK for entity extraction. \n",
    "- Firstly, install python environment\n",
    "- Change the source mirror of pip : https://mirrors.bfsu.edu.cn/help/pypi/\n",
    "- Install NLTK: pip install nltk\n",
    "- Download data distribution for NLTK. Enter python terminal first. import nltk. Install packages by using NLTK downloader: ``nltk.download()``. If cannot download using ``nltk.download()``, try download manually from https://github.com/nltk/nltk_data/tree/gh-pages![image.png](attachment:image.png) or https://pan.baidu.com/s/1wONWpaa86_wnsIksKda8eQ (code:tfon )\n",
    "- Unzip the downloaded file to the following folder: ``nltk.data.find(\".\")``\n",
    "- Unzip each zip file in the ten folders: *chunkers, corpora, grammers, help, misc, models, sentiment, stemmers, taggers, tokenizers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages \n",
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag, ne_chunk\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'was',\n",
       " 'born',\n",
       " 'in',\n",
       " 'Liverpool',\n",
       " ',',\n",
       " 'to',\n",
       " 'Julia',\n",
       " 'and',\n",
       " 'Alfred',\n",
       " 'Lennon']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentence:\n",
    "raw = \"\"\"John was born in Liverpool, to Julia and Alfred Lennon\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Liverpool', 'NNP'), (',', ','), ('to', 'TO'), ('Julia', 'NNP'), ('and', 'CC'), ('Alfred', 'NNP'), ('Lennon', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# pos-tag of inputs\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know the detail information of each tag, use the following statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking:\n",
    "- Use ``ne_chunk`` provided by NLTK. ``ne_chunk`` needs part-of-speech annotations to add ``NE`` labels to the sentence. The output of the ``ne_chunk`` is a ``nltk.Tree`` object\n",
    "- ``ne_chunk`` produces 2-level trees:\n",
    " - Nodes on Level-1: outsides any chunk\n",
    " - Nodes on Level-2: inside a chunk (the label of the chunk is denoted by the label of the subtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Liverpool/NNP)\n",
      "  ,/,\n",
      "  to/TO\n",
      "  (GPE Julia/NNP)\n",
      "  and/CC\n",
      "  (PERSON Alfred/NNP Lennon/NNP))\n"
     ]
    }
   ],
   "source": [
    "chunks = ne_chunk(pos_tag(word_tokenize(raw)))\n",
    "print(chunks)\n",
    "chunks.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverse the chunked tree structure to get each chunk and words inside each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON John/NNP) <class 'nltk.tree.tree.Tree'>\n",
      "Chunk detect!\n",
      "John NNP\n",
      "('was', 'VBD') <class 'tuple'>\n",
      "('born', 'VBN') <class 'tuple'>\n",
      "('in', 'IN') <class 'tuple'>\n",
      "(GPE Liverpool/NNP) <class 'nltk.tree.tree.Tree'>\n",
      "Chunk detect!\n",
      "Liverpool NNP\n",
      "(',', ',') <class 'tuple'>\n",
      "('to', 'TO') <class 'tuple'>\n",
      "(GPE Julia/NNP) <class 'nltk.tree.tree.Tree'>\n",
      "Chunk detect!\n",
      "Julia NNP\n",
      "('and', 'CC') <class 'tuple'>\n",
      "(PERSON Alfred/NNP Lennon/NNP) <class 'nltk.tree.tree.Tree'>\n",
      "Chunk detect!\n",
      "Alfred NNP\n",
      "Lennon NNP\n"
     ]
    }
   ],
   "source": [
    "for i in chunks:\n",
    "    print(i, type(i))\n",
    "    if type(i) == Tree:\n",
    "        print('Chunk detect!')\n",
    "        chunk_phrase = []\n",
    "        for token,pos in i.leaves():\n",
    "            print(token,pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise1\n",
    "Extract all named entities as well as its type/label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'John': 'PERSON',\n",
       " 'Liverpool': 'GPE',\n",
       " 'Julia': 'GPE',\n",
       " 'Alfred Lennon': 'PERSON'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all packages \n",
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag, ne_chunk\n",
    "from nltk import Tree\n",
    "\n",
    "# Exercise1, define a function to extract all named enties together with labels\n",
    "def get_labeled_chunks(text):\n",
    "    # your implementation\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    # getting the chunks\n",
    "    chunks = ne_chunk(tagged)\n",
    "    label_entities={}\n",
    "    # read the chunks line by line\n",
    "    for i in chunks:\n",
    "        #print(i)\n",
    "        #detect the if the type is tree, if yes it maybe name\n",
    "        if type(i) == Tree:\n",
    "            chunk_phrase = []\n",
    "            # if it is person or GPE, it will record\n",
    "            if(i.label()==\"PERSON\" or i.label()==\"GPE\"):\n",
    "                #record the label\n",
    "                name_label=i.label()\n",
    "                # append the whole name if it have multiply part\n",
    "                namelist=[]\n",
    "                for token,pos in i.leaves():\n",
    "                    namelist.append(token)\n",
    "            name=\" \".join(namelist)\n",
    "            # append it to the dictionary\n",
    "            label_entities[name]=name_label\n",
    "    return label_entities\n",
    "get_labeled_chunks(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise2\n",
    "Extract only *PERSON* entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'Alfred Lennon']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise2, extract all the entities of specific type\n",
    "def get_type_chunks(text,label1):\n",
    "    # your implementation\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    # getting the chunks\n",
    "    chunks = ne_chunk(tagged)\n",
    "    entity=[]\n",
    "    # read the chunks line by line\n",
    "    for i in chunks:\n",
    "        #print(i)\n",
    "        #detect the if the type is tree, if yes it maybe name\n",
    "        if type(i) == Tree:\n",
    "            chunk_phrase = []\n",
    "            # if it is person or GPE, it will record\n",
    "            if(i.label()==label1):\n",
    "                # append the whole name if it have multiply part\n",
    "                name_len=[]\n",
    "                for token,pos in i.leaves():\n",
    "                    name_len.append(token)\n",
    "                name=\" \".join(name_len)\n",
    "                entity.append(name)\n",
    "    return entity\n",
    "get_type_chunks(raw,'PERSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise3: Noun phrase chunking\n",
    "Define your own grammer for noun phrase chunking using ``nltk.RegexpParser``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little dog', 'cat']\n",
      "['Jonh', 'Liverpool', 'Julia', 'Alfred Lennon']\n"
     ]
    }
   ],
   "source": [
    "def np_chunking(sentence):\n",
    "    # define a grammer then build my own chunk\n",
    "    grammer = \"NP:{<JJ>*<NN.*>+}\\n {<NN.*>+}\"\n",
    "    cp = nltk.RegexpParser(grammer)\n",
    "    mychunk = cp.parse(pos_tag(word_tokenize(sentence)))\n",
    "    #print(mychunk)\n",
    "    entity=[]\n",
    "    # read line by line\n",
    "    for i in mychunk:\n",
    "        if type(i) == Tree:\n",
    "            #detect the NP\n",
    "            if(i.label()==\"NP\"):\n",
    "                name_len=[]\n",
    "                for token,pos in i.leaves():\n",
    "                    name_len.append(token)\n",
    "                name=\" \".join(name_len)\n",
    "                entity.append(name)\n",
    "    return entity\n",
    "\n",
    "print(np_chunking(\"\"\"the little dog barked at the cat\"\"\"))\n",
    "print(np_chunking(\"\"\"Jonh was born in Liverpool, to Julia and Alfred Lennon\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
