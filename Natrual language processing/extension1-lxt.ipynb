{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom transformers import AdamW\nfrom datasets import list_metrics,load_metric\nfrom sklearn.metrics import confusion_matrix\nimport plotly.express as px\nfrom pylab import *\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:45.537596Z","iopub.execute_input":"2022-05-29T13:59:45.537962Z","iopub.status.idle":"2022-05-29T13:59:45.557014Z","shell.execute_reply.started":"2022-05-29T13:59:45.537928Z","shell.execute_reply":"2022-05-29T13:59:45.555165Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Dataset reading\nWe seperating the dataset line by lline with the corresponding features","metadata":{}},{"cell_type":"code","source":"def read_data(path):\n    sentence_list = []\n    e12_list = []\n    label_list = []\n    comment_list = []\n    ID_list = []\n    count=0\n    tag_list = [\"<e1>\",\"</e1>\",\"<e2>\",\"</e2>\"]\n    with open(path, \"r\") as f:\n        lines = f.readlines()\n        for line_index in range(0,len(lines),4):\n            # append id\n            ID_list.append(int(line_index/4+1))\n            \n            # append e12\n            try:\n                e1 = lines[line_index][:-1].split(\"<e1>\")[1].split(\"</e1>\")[0]\n                e2 = lines[line_index][:-1].split(\"<e2>\")[1].split(\"</e2>\")[0]\n                e12_list.append([e1,e2])\n            except:\n                print(setence)\n                return 0\n            \n            # append sentence\n            setence = (lines[line_index][:-1].split(\"\\t\")[1])[1:-1]\n            for tag in tag_list:\n                setence = setence.replace(tag, '')\n                \n            sentence_list.append(setence)\n            \n            # append label and comment\n            label_list.append(lines[line_index+1][:-1])\n            comment_list.append(lines[line_index+2][9:-1])\n#             count+=1\n#             if count>100:\n#                 print(ID_list)\n#                 print(e12_list)\n#                 print(sentence_list)\n#                 print(comment_list)\n#                 print(label_list)\n#                 return 0\n        return ID_list,sentence_list,e12_list,label_list,comment_list","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:45.560376Z","iopub.execute_input":"2022-05-29T13:59:45.561606Z","iopub.status.idle":"2022-05-29T13:59:45.575898Z","shell.execute_reply.started":"2022-05-29T13:59:45.561551Z","shell.execute_reply":"2022-05-29T13:59:45.574962Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_path = \"../input/text-classification/TRAIN_FILE.TXT\"\ntest_path = \"../input/text-classification/FULL_TEST.txt\"\nID_list,sentence_list,e12_list,label_list,comment_list = read_data(train_path)\nID_list_test,sentence_list_test,e12_list_test,label_list_test,comment_list_test = read_data(test_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:45.580063Z","iopub.execute_input":"2022-05-29T13:59:45.580496Z","iopub.status.idle":"2022-05-29T13:59:45.716085Z","shell.execute_reply.started":"2022-05-29T13:59:45.580460Z","shell.execute_reply":"2022-05-29T13:59:45.715351Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The following are the feature we make:","metadata":{}},{"cell_type":"code","source":"display(ID_list[:3])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:45.717619Z","iopub.execute_input":"2022-05-29T13:59:45.718641Z","iopub.status.idle":"2022-05-29T13:59:45.729014Z","shell.execute_reply.started":"2022-05-29T13:59:45.718594Z","shell.execute_reply":"2022-05-29T13:59:45.728085Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"display(sentence_list[:3])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:45.730340Z","iopub.execute_input":"2022-05-29T13:59:45.731415Z","iopub.status.idle":"2022-05-29T13:59:45.945243Z","shell.execute_reply.started":"2022-05-29T13:59:45.731368Z","shell.execute_reply":"2022-05-29T13:59:45.944596Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"display(e12_list[:3])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:45.946303Z","iopub.execute_input":"2022-05-29T13:59:45.946967Z","iopub.status.idle":"2022-05-29T13:59:45.958062Z","shell.execute_reply.started":"2022-05-29T13:59:45.946931Z","shell.execute_reply":"2022-05-29T13:59:45.957325Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"display(label_list[:3])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:45.958986Z","iopub.execute_input":"2022-05-29T13:59:45.959643Z","iopub.status.idle":"2022-05-29T13:59:45.970360Z","shell.execute_reply.started":"2022-05-29T13:59:45.959611Z","shell.execute_reply":"2022-05-29T13:59:45.969653Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"display(comment_list[:3])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:45.971520Z","iopub.execute_input":"2022-05-29T13:59:45.971939Z","iopub.status.idle":"2022-05-29T13:59:45.982045Z","shell.execute_reply.started":"2022-05-29T13:59:45.971900Z","shell.execute_reply":"2022-05-29T13:59:45.981451Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"classes = list(set(label_list))\nlen(classes)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-29T13:59:45.983539Z","iopub.execute_input":"2022-05-29T13:59:45.984091Z","iopub.status.idle":"2022-05-29T13:59:45.994918Z","shell.execute_reply.started":"2022-05-29T13:59:45.984048Z","shell.execute_reply":"2022-05-29T13:59:45.994012Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"dic = {}\ndic_rev = {}\nlabel_listing = range(len(classes))\nfor i,j in zip(classes,label_listing):\n    dic_rev[i] = j\n    dic[j] = i\ndisplay(dic)\ndisplay(dic_rev)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:45.997290Z","iopub.execute_input":"2022-05-29T13:59:45.997766Z","iopub.status.idle":"2022-05-29T13:59:46.011452Z","shell.execute_reply.started":"2022-05-29T13:59:45.997721Z","shell.execute_reply":"2022-05-29T13:59:46.010371Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Data type convert","metadata":{}},{"cell_type":"code","source":"train_set = []\nfor i in range(len(sentence_list)):\n    label = label_list[i]\n    train_set.append([(\" \".join(e12_list[i]),sentence_list[i]),dic_rev.get(label)])\ntrain_set[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:46.014114Z","iopub.execute_input":"2022-05-29T13:59:46.014568Z","iopub.status.idle":"2022-05-29T13:59:46.033788Z","shell.execute_reply.started":"2022-05-29T13:59:46.014527Z","shell.execute_reply":"2022-05-29T13:59:46.033059Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_set = []\nfor i in range(len(sentence_list_test)):\n    label = label_list_test[i]\n    test_set.append([(\" \".join(e12_list_test[i]),sentence_list_test[i]),dic_rev.get(label)])\ntest_set[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:46.035323Z","iopub.execute_input":"2022-05-29T13:59:46.035773Z","iopub.status.idle":"2022-05-29T13:59:46.051063Z","shell.execute_reply.started":"2022-05-29T13:59:46.035728Z","shell.execute_reply":"2022-05-29T13:59:46.050129Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(\"Total number of the training set is\",len(ID_list))\nprint(\"Total number of the testing set is\",len(ID_list_test))","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:59:46.454931Z","iopub.execute_input":"2022-05-29T13:59:46.455563Z","iopub.status.idle":"2022-05-29T13:59:46.460549Z","shell.execute_reply.started":"2022-05-29T13:59:46.455520Z","shell.execute_reply":"2022-05-29T13:59:46.459760Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Named entity type\nIn this project, I will finish the task of named entity type, which is return the prediction for the labeled entity. ","metadata":{}},{"cell_type":"markdown","source":"Example:","metadata":{}},{"cell_type":"markdown","source":"\"**American Airlines**, a unit of AMR, immediately matched the move, spokesman **Tim Wagner** said\" \n\n1. E1: ORGANIZATION\n\n2. E2: PERSON\n\n3. E1+E2: ORGANIZATION-PERSON","metadata":{}},{"cell_type":"markdown","source":"## loading the tokenizer","metadata":{}},{"cell_type":"code","source":"token = BertTokenizer.from_pretrained('bert-large-uncased')\ntoken","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:27:29.884443Z","iopub.execute_input":"2022-05-29T13:27:29.885034Z","iopub.status.idle":"2022-05-29T13:27:38.220305Z","shell.execute_reply.started":"2022-05-29T13:27:29.884985Z","shell.execute_reply":"2022-05-29T13:27:38.219448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this place, i treat the data as a QA question, input is [(sentence,keyword),answer] for each of the data.","metadata":{}},{"cell_type":"code","source":"# collate function\ndef collate_fn(data):\n    # split the sentences and labels\n    sents = [i[0] for i in data]\n    labels = [i[1] for i in data]\n\n    # encoding, we loading the token of chinese\n    # Batch coding\n    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,  # this is the sentences \n                                   truncation=True,                 # if exceed the max length, it will be cut\n                                   padding='max_length',            # padding zero to the max length\n                                   max_length=150,                   # max length of the sentence\n                                   return_tensors='pt',             # the type (pythorch or tensorflow) used pt\n                                   return_length=True)              # it will contain the length      \n\n    # the number after the encoding\n    input_ids = data['input_ids']\n    # the place where we padding 0 is 0, other with orginial data is 1\n    attention_mask = data['attention_mask']\n    # since every sentence is one sentence, everything will be 0\n    token_type_ids = data['token_type_ids']\n    # modify the type of labels\n    labels = torch.LongTensor(labels)\n#     print(data)\n#     print(data['length'], data['length'].max())\n#     print(labels)\n    return input_ids, attention_mask, token_type_ids, labels\n\n# Using small batch training tends to converge to flat minimization\nbatch_size = 16\n\n# data loader\nloader = torch.utils.data.DataLoader(dataset=train_set,             # the input is the training set\n                                     batch_size=batch_size,         # the number of data samples captured in one training\n                                     collate_fn=collate_fn,         # use the collate function(Merge the data and labels of a batch)\n                                     shuffle=True,                  # mix the data\n                                     drop_last=True)                # Delete the incomplete last batch\n\n# view the data , token_type_ids\nfor i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n    break\n\nprint(len(loader))\ninput_ids.shape, attention_mask.shape , token_type_ids.shape, labels","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:27:38.221714Z","iopub.execute_input":"2022-05-29T13:27:38.22212Z","iopub.status.idle":"2022-05-29T13:27:38.255269Z","shell.execute_reply.started":"2022-05-29T13:27:38.222084Z","shell.execute_reply":"2022-05-29T13:27:38.254472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the pre-trained model\npretrained = BertModel.from_pretrained('bert-large-uncased')\n\n# # we just need to use it but not train it\n# for param in pretrained.parameters():\n#     param.requires_grad_(True)\n\n# test for the pretrained data\nout = pretrained(input_ids=input_ids,\n           attention_mask=attention_mask,\n           token_type_ids=token_type_ids)\n\n# 16 batch size for the 200 max length, 1024 is the dimension of encoding\nout.last_hidden_state.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:27:38.25665Z","iopub.execute_input":"2022-05-29T13:27:38.257207Z","iopub.status.idle":"2022-05-29T13:29:55.50289Z","shell.execute_reply.started":"2022-05-29T13:27:38.257172Z","shell.execute_reply":"2022-05-29T13:29:55.501903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"next(iter(loader))[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:50:05.254769Z","iopub.execute_input":"2022-05-29T13:50:05.255148Z","iopub.status.idle":"2022-05-29T13:50:05.282177Z","shell.execute_reply.started":"2022-05-29T13:50:05.255116Z","shell.execute_reply":"2022-05-29T13:50:05.281339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First take the pre training model for calculation, extract the features from the data, \n# and then put the features into the fully connected neural network for calculation\n\n# Feature extraction based on pre training\n\n# Transfer learning through downstream tasks\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Single layer of totally connected neural network(2 means the classes number)\n        self.device1 = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.device2 = torch.device(\"cuda\")\n        self.model = pretrained\n        self.decoder = torch.nn.Linear(1024, 19)\n        self.decoder.to(self.device1)\n                        \n#         self.bias = torch.nn.Parameter(torch.zeros(token.vocab_size))\n#         self.bias.to(self.device1)\n#         print(self.bias.device)\n#         self.decoder.bias = self.bias\n#         self.decoder.bias.to(self.device2)\n#         print(self.decoder.bias.device)\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        input_ids, attention_mask, token_type_ids = input_ids.to(self.device1), attention_mask.to(self.device1), token_type_ids.to(self.device1)\n        pretrained.to(self.device1)\n        out = pretrained(input_ids=input_ids,\n                       attention_mask=attention_mask,\n                       token_type_ids=token_type_ids)\n        # Only the features of the 0th word need to be used\n        # [cls] is used for classification tasks and appears at the 0th index of the bert output\n        out = self.decoder(out.last_hidden_state[:, 0])\n\n        #out = out.softmax(dim=1)\n\n        return out\n\nmodel = Model()\n\nmodel(input_ids=input_ids,\n      attention_mask=attention_mask,\n      token_type_ids=token_type_ids).shape","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:50:06.174949Z","iopub.execute_input":"2022-05-29T13:50:06.175838Z","iopub.status.idle":"2022-05-29T13:50:06.277544Z","shell.execute_reply.started":"2022-05-29T13:50:06.175779Z","shell.execute_reply":"2022-05-29T13:50:06.276181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.parameters","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:50:07.118648Z","iopub.execute_input":"2022-05-29T13:50:07.119008Z","iopub.status.idle":"2022-05-29T13:50:07.130137Z","shell.execute_reply.started":"2022-05-29T13:50:07.118963Z","shell.execute_reply":"2022-05-29T13:50:07.129331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training the model\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# cross entropy loss as the criterion\ncriterion = torch.nn.CrossEntropyLoss()\n\nx=[]\ny_l=[]\ny_a=[]\n\nepochs = 50\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.train()\ncount = 1\nfor epoch in range(epochs):\n    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n        \n#         input_ids = input_ids.to(device)\n#         attention_mask = attention_mask.to(device)\n#         token_type_ids = token_type_ids.to(device)\n        labels = labels.to(device)\n        \n        # get the output\n        out = model(input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_type_ids=token_type_ids)\n        \n        out = out.to(device)\n        \n        # get the loss and minimize it\n        loss = criterion(out, labels)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        print(i)\n        # save the model for every 16 times\n        if i % 10 == 0:\n#             out = out.to(torch.device(\"cpu\"))\n            out = out.argmax(dim=1)\n            accuracy = (out == labels).sum().item() / len(labels)\n            torch.save(model,\"./bert_model.pt\")\n\n            # for each time, print the accuracy\n            print(i, loss.item(), accuracy)\n\n            x.append(count)\n            y_l.append(loss.item())\n            y_a.append(accuracy)\n            count += 1\n\n# \n#         if i == 6:\n#            break\n#     break","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:30:01.78554Z","iopub.execute_input":"2022-05-29T13:30:01.785924Z","iopub.status.idle":"2022-05-29T13:30:53.356616Z","shell.execute_reply.started":"2022-05-29T13:30:01.785886Z","shell.execute_reply":"2022-05-29T13:30:53.355307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ploting(name,x,y):\n    plt.rcParams['figure.figsize'] = (12.0, 8.0) \n    plt.plot(x, y, 'r-', alpha=0.8, label=name)\n    plt.legend(loc=\"upper right\")\n    plt.xlabel('iter')\n    plt.ylabel(name)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:30:53.357715Z","iopub.status.idle":"2022-05-29T13:30:53.360371Z","shell.execute_reply.started":"2022-05-29T13:30:53.360121Z","shell.execute_reply":"2022-05-29T13:30:53.360148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name_a = \"Accuary\"\nname_l = \"Loss\"\nploting(name_a,x,y_a)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:30:53.36169Z","iopub.status.idle":"2022-05-29T13:30:53.36216Z","shell.execute_reply.started":"2022-05-29T13:30:53.361912Z","shell.execute_reply":"2022-05-29T13:30:53.361935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ploting(name_l,x,y_l)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:30:53.363638Z","iopub.status.idle":"2022-05-29T13:30:53.364178Z","shell.execute_reply.started":"2022-05-29T13:30:53.363897Z","shell.execute_reply":"2022-05-29T13:30:53.363924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"#print(load_metric(\"precision\").inputs_description)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:46:24.6222Z","iopub.execute_input":"2022-05-29T10:46:24.622844Z","iopub.status.idle":"2022-05-29T10:46:24.626579Z","shell.execute_reply.started":"2022-05-29T10:46:24.622808Z","shell.execute_reply":"2022-05-29T10:46:24.625563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def performance(y_ture,y_pred):\n    f1_metric = load_metric(\"f1\")\n    re_metric = load_metric(\"recall\")\n    pre_metric = load_metric(\"precision\")\n    type_c_int = list(set(np.concatenate([y_ture, y_pred])))\n    type_c = [str(i) for i in type_c_int]\n    \n    f1_m_list = []\n    re_m_list = []\n    pre_m_list = []\n    \n    for i in type_c_int:\n        bi_ture = list(y_ture == i)\n        bi_pred = list(y_pred == i)\n        f1_m_results = f1_metric.compute(predictions=bi_pred, references=bi_ture, average=\"macro\")\n        re_m_results = re_metric.compute(predictions=bi_pred, references=bi_ture, average=\"macro\")\n        pre_m_results = pre_metric.compute(predictions=bi_pred, references=bi_ture, average=\"macro\")\n        \n        f1_m_list.append(f1_m_results[\"f1\"])\n        re_m_list.append(re_m_results[\"recall\"])\n        pre_m_list.append(pre_m_results[\"precision\"])\n        \n    data = {'Class_type':type_c_int,'F1-macro':f1_m_list,'Recall-macro':re_m_list,'Precision-macro':pre_m_list}\n    df = pd.DataFrame(data)\n    display(df)\n    \n    \n    z = confusion_matrix(y_ture, y_pred)\n    x_lab = type_c\n\n    fig = px.imshow(z, \n                    text_auto=True,\n                    labels=dict(x=\"True label\", y=\"Predicted label\", color=\"times\"),\n                    x=x_lab,\n                    y=x_lab)\n    fig.show()\n    \n    return z\n    \ncf_matrix_test = performance([1,3,1,4,2,1],[2,3,1,3,3,2])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:46:24.628238Z","iopub.execute_input":"2022-05-29T10:46:24.628887Z","iopub.status.idle":"2022-05-29T10:46:30.819467Z","shell.execute_reply.started":"2022-05-29T10:46:24.628831Z","shell.execute_reply":"2022-05-29T10:46:30.818731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model,if_load):\n    y_pred = []\n    y_true = []\n    if(if_load):\n        model = torch.load('/kaggle/input/model-pt/bert_model.pt')\n    try:\n        model.eval()\n    except:\n        return -1\n    correct = 0\n    total = 0\n    \n    # loading the test set\n    loader_test = torch.utils.data.DataLoader(dataset=test_set,\n                                              batch_size=32,\n                                              collate_fn=collate_fn,\n                                              shuffle=True,\n                                              drop_last=True)\n    \n    # use for each\n    for i, (input_ids, attention_mask, token_type_ids,labels) in enumerate(loader_test):\n\n#         if i == 2:\n#             break\n        if i % 10 == 0:\n            print(i)\n        # get the output by the model\n        with torch.no_grad():\n            out = model(input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids)\n        \n        out = out.to(device)\n        \n        # get the argument max\n        out = out.argmax(dim=1)\n        # calculate the correct number and the total number\n        out_list = out.tolist()\n        labels_list = labels.tolist()\n        y_pred.extend(out_list)\n        y_true.extend(labels_list)\n    # print the final Result\n    return y_true,y_pred\n\nif_load = 0\ny_t,y_p = test(model,if_load)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:47:09.899515Z","iopub.execute_input":"2022-05-29T10:47:09.899899Z","iopub.status.idle":"2022-05-29T10:47:10.920476Z","shell.execute_reply.started":"2022-05-29T10:47:09.899857Z","shell.execute_reply":"2022-05-29T10:47:10.919648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf_matrix = performance(y_t,y_p)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T10:46:31.857089Z","iopub.execute_input":"2022-05-29T10:46:31.85746Z","iopub.status.idle":"2022-05-29T10:46:35.862786Z","shell.execute_reply.started":"2022-05-29T10:46:31.857424Z","shell.execute_reply":"2022-05-29T10:46:35.862072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}